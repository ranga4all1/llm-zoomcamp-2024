{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6811167-1218-42af-a330-9119ce73ee28",
   "metadata": {},
   "source": [
    "## LLM on a CPU - ollama\n",
    "\n",
    "```\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "ollama start\n",
    "ollama pull phi3\n",
    "ollama run phi3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f216b47-6b7a-43be-ade6-2f85171eb2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b23c46-60cf-4683-b9fe-fb68f8259cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506fab2a-a50c-42bd-a106-c83a9d2828ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-03 18:06:17--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-07-03 18:06:17 (13.9 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac947de-effd-4b61-8792-a6d7a133f347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x74b8a2116470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f087272-b44d-4738-9ea2-175ec63a058b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "742ab881-499a-4675-83c4-2013ea1377b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        # model='gpt-4o',\n",
    "        # Set model to phi3 running on local ollama\n",
    "        model='phi3',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8bff3e-b672-42be-866b-f2d9bb217106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d155feaf-4b3e-4422-aae2-4d4f2accaac9",
   "metadata": {},
   "source": [
    "### ollama is drop in replacement for OPENAI. Let's connect to local ollama instance now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "147b6594-ef2a-4459-9581-aba6d738112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ac9331-f9e0-49e7-bec1-0c72c0e084fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, but it seems you may have accidentally triggered an AI response instead. To clarify the purpose of your inquiry and provide appropriate assistance or information, could you please rephrase as such? It would be most helpful if you could describe what specific topic, idea, question, or subject this is supposed to address in a clearer manner for me to assist effectively.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('Write that this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d8fae6b-d804-4069-a78e-5615534ef411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry, but it seems you may have accidentally triggered an AI response instead. To clarify the purpose of your inquiry and provide appropriate assistance or information, could you please rephrase as such? It would be most helpful if you could describe what specific topic, idea, question, or subject this is supposed to address in a clearer manner for me to assist effectively.\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac7281-67ed-433e-ae49-32a7d4477d06",
   "metadata": {},
   "source": [
    "### ollama in Docker\n",
    "\n",
    "```\n",
    "docker run -it \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "* Pull and Run Phi3 in ollama inside docker\n",
    "\n",
    "```\n",
    "docker exec -it ollama bash\n",
    "ollama pull phi3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6abcdd36-223c-45b8-8930-e7c0d6597a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is a simple text-based instruction, asking for the content to be as follows: \"This is a test.\" It implies creating or displaying this exact phrase. An output would simply involve typing out or visualizing these words on screen, paper, etc., with no additional information required beyond confirming that such content has been generated successfully and accurately reflects what was instructed – essentially conducting an internal check to ensure the text \"This is a test\" appears as expected before any real testing procedures would begin.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('Write that this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d3a50c3-25b1-4442-9381-4db768523326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a simple text-based instruction, asking for the content to be as follows: \"This is a test.\" It implies creating or displaying this exact phrase. An output would simply involve typing out or visualizing these words on screen, paper, etc., with no additional information required beyond confirming that such content has been generated successfully and accurately reflects what was instructed – essentially conducting an internal check to ensure the text \"This is a test\" appears as expected before any real testing procedures would begin.\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7228882-af87-4ccf-8911-283e953c598a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
